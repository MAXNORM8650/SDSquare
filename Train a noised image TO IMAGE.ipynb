{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09935cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTrain a noised image classifier on ImageNet.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a noised image classifier on ImageNet.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f7974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\".\")\n",
    "from guided_diffusion.bratsloader import BRATSDataset\n",
    "import blobfile as bf\n",
    "import torch as th\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from visdom import Visdom\n",
    "import numpy as np\n",
    "viz = Visdom(port=8097)\n",
    "loss_window = viz.line( Y=th.zeros((1)).cpu(), X=th.zeros((1)).cpu(), opts=dict(xlabel='epoch', ylabel='Loss', title='classification loss'))\n",
    "val_window = viz.line( Y=th.zeros((1)).cpu(), X=th.zeros((1)).cpu(), opts=dict(xlabel='epoch', ylabel='Loss', title='validation loss'))\n",
    "acc_window= viz.line( Y=th.zeros((1)).cpu(), X=th.zeros((1)).cpu(), opts=dict(xlabel='epoch', ylabel='acc', title='accuracy'))\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.fp16_util import MixedPrecisionTrainer\n",
    "from guided_diffusion.image_datasets import load_data\n",
    "from guided_diffusion.train_util import visualize\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "from guided_diffusion.script_util import (\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    classifier_and_diffusion_defaults,\n",
    "    create_classifier_and_diffusion,\n",
    ")\n",
    "from guided_diffusion.train_util import parse_resume_step_from_filename, log_loss_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f284fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "  \"image_size\": 256,\n",
    "  \"Batch_Size\": 8,\n",
    "  \"batch_size\": 8,\n",
    "  \"channels\": 3,\n",
    "  \"EPOCHS\": 4000,\n",
    "  \"iterations\":40000,\n",
    "  \"diffusion_steps\": 1000,\n",
    "  \"num_channels\": 192,\\\n",
    "  \"learn_sigma\":True,\n",
    "  \"val_data\": True,\n",
    "  \"eval_interval\": 1000,\n",
    "  \"save_interval\": 1000,\n",
    "  \"log_interval\": 10,\n",
    "  \"anneal_lr\": True,\n",
    "  \"microbatch\": -1,\n",
    "  \"use_kl\":False,\n",
    "  \"schedule_sampler\": \"uniform\",\n",
    "  \"resume_checkpoint\": False,\n",
    "  \"use_checkpoint\":False,\n",
    "  \"predict_xstart\":True,\n",
    "  \"rescale_timesteps\":False,\n",
    "  \"rescale_learned_sigmas\":False,\n",
    "  \"timestep_respacing\": \"\",\n",
    "  \"noise_schedule\": \"cosine\",\n",
    "  \"channel_mults\": \"\",\n",
    "  \"channel_mult\": \"\",\n",
    "  \"use_new_attention_order\": False,\n",
    "  \"loss-type\": \"l2\",\n",
    "  \"loss_weight\": \"none\",\n",
    "  \"train_start\": True,\n",
    "  \"lr\": 1e-4,\n",
    "  \"learn_sigma \": True,\n",
    "  \"random_slice\": True,\n",
    "  \"sample_distance\": 800,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"save_imgs\": True,\n",
    "  \"save_vids\": True,\n",
    "  \"class_cond\": True,\n",
    "  \"use_fp16\": True,\n",
    "  \"use_scale_shift_norm\": True,\n",
    "  \"dropout\": 0.1,\n",
    "  \"attention_resolutions\": \"32,16,8\",\n",
    "  \"num_heads\": 1,\n",
    "  \"num_heads_upsample\":-1,\n",
    "  \"num_res_blocks\": 3,\n",
    "  \"resblock_updown\": True,\n",
    "  \"classifier_use_fp16\":False,\n",
    "  \"classifier_width\":128,\n",
    "  \"classifier_depth\": 2,\n",
    "  \"classifier_attention_resolutions\": \"32,16,8\",\n",
    "  \"classifier_use_scale_shift_norm\": True,\n",
    "  \"classifier_resblock_updown\": True,\n",
    "  \"classifier_pool\": \"attention\" ,\n",
    "  \"num_head_channels\": 64,\n",
    "  \"noised\": True,\n",
    "  \"noise_fn\": \"gauss\",\n",
    "  \"dataset\": \"brats\",\n",
    "  \"data_dir\":r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\Anomaly Detection\\AnoDDPM\\DATASETS\\brats2021\\traning_data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c52729f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = dict(\n",
    "    schedule_sampler=\"uniform\",\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.0,\n",
    "    lr_anneal_steps=0,\n",
    "    batch_size=1,\n",
    "    microbatch=-1,  # -1 disables microbatches\n",
    "    ema_rate=\"0.9999\",  # comma-separated list of EMA values\n",
    "    log_interval=100,\n",
    "    save_interval=10000,\n",
    "    resume_checkpoint='',\n",
    "    use_fp16=False,\n",
    "    fp16_scale_growth=1e-3,\n",
    "    dataset='brats',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dfac97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict.update(defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0e6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7dab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args_dict)\n",
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f030bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR= './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69463b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38b3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d64b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b559a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, testing_dataset = dataset.init_datasets(ROOT_DIR, args_dict)\n",
    "datal = dataset.init_dataset_loader(training_dataset, args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e403261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n",
      "torch.Size([4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i in training_dataset:\n",
    "    print(i[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce30091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cd8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import nibabel\n",
    "from scipy import ndimage\n",
    "\n",
    "class BRATSDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory, test_flag=False):\n",
    "        '''\n",
    "        directory is expected to contain some folder structure:\n",
    "                  if some subfolder contains only files, all of these\n",
    "                  files are assumed to have a name like\n",
    "                  brats_train_001_XXX_123_w.nii.gz\n",
    "                  where XXX is one of t1, t1ce, t2, flair, seg\n",
    "                  we assume these five files belong to the same image\n",
    "                  seg is supposed to contain the segmentation\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.directory = os.path.expanduser(directory)\n",
    "\n",
    "        self.test_flag=test_flag\n",
    "        if test_flag:\n",
    "            self.seqtypes = ['t1', 't1ce', 't2', 'flair']\n",
    "        else:\n",
    "            self.seqtypes = ['t1', 't1ce', 't2', 'flair', 'seg']\n",
    "\n",
    "        self.seqtypes_set = set(self.seqtypes)\n",
    "        self.database = []\n",
    "        for root, dirs, files in os.walk(self.directory):\n",
    "            # if there are no subdirs, we have data\n",
    "            if not dirs:\n",
    "                files.sort()\n",
    "                datapoint = dict()\n",
    "                # extract all files as channels\n",
    "                for f in files:\n",
    "                    seqtype = f[16:-7]\n",
    "                    datapoint[seqtype] = os.path.join(root, f)\n",
    "                assert set(datapoint.keys()) == self.seqtypes_set, \\\n",
    "                    f'datapoint is incomplete, keys are {datapoint.keys()}'\n",
    "                self.database.append(datapoint)\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        out = []\n",
    "        filedict = self.database[x]\n",
    "#         print(filedict)\n",
    "        for seqtype in self.seqtypes:\n",
    "            number=filedict['t1'].split('\\\\')[-3][10:]\n",
    "            nib_img = nibabel.load(filedict[seqtype])\n",
    "            out.append(torch.tensor(nib_img.get_fdata()))\n",
    "        out = torch.stack(out)\n",
    "        print(out.shape)\n",
    "        out_dict = {}\n",
    "        if self.test_flag:\n",
    "            path2 = './data/brats/test_labels/' + str(\n",
    "                number) + '-label.nii.gz'\n",
    "\n",
    "\n",
    "            seg=nibabel.load(path2)\n",
    "            seg=seg.get_fdata()\n",
    "            image = torch.zeros(4, 256, 256)\n",
    "            image[:, 8:-8, 8:-8] = out\n",
    "            label = seg[None, ...]\n",
    "            if seg.max() > 0:\n",
    "                weak_label = 1\n",
    "            else:\n",
    "                weak_label = 0\n",
    "            out_dict[\"y\"]=weak_label\n",
    "        else:\n",
    "            image = torch.zeros(4,256,256)\n",
    "            image[:,8:-8,8:-8]=out[:-1,...]\t\t#pad to a size of (256,256)\n",
    "            label = out[-1, ...][None, ...]\n",
    "            if label.max()>0:\n",
    "                weak_label=1\n",
    "            else:\n",
    "                weak_label=0\n",
    "            out_dict[\"y\"] = weak_label\n",
    "\n",
    "        return (image, out_dict, weak_label, label, number )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.database)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a457498",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BRATSDataset(args.data_dir, test_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b86e5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "datal = th.utils.data.DataLoader(ds, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0004be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\Anomaly Detection\\AnoDDPM\\DATASETS\\Train\\A00035840\\sub-A00035840_ses-NFB3_T1w_brain.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fe3b4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 192])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nib_img = nibabel.load(path)\n",
    "torch.tensor(nib_img.get_fdata()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd00c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e3379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5c6d688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 240, 240, 155])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m datal:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[19], line 72\u001b[0m, in \u001b[0;36mBRATSDataset.__getitem__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mout[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\t\t\u001b[38;5;66;03m#pad to a size of (256,256)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     label \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "for i in datal:\n",
    "    print(i[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "738524a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randint\n",
    "\n",
    "import cv2\n",
    "import nibabel\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import animation\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import imageio\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import ndimage\n",
    "class MRIDataset(Dataset):\n",
    "    \"\"\"Healthy MRI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, ROOT_DIR, transform=None, img_size=(32, 32), random_slice=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ROOT_DIR (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transforms.Compose(\n",
    "                [transforms.ToPILImage(),\n",
    "                 transforms.RandomAffine(3, translate=(0.02, 0.09)),\n",
    "                 transforms.CenterCrop(235),\n",
    "                 transforms.Resize(img_size, transforms.InterpolationMode.BILINEAR),\n",
    "                 # transforms.CenterCrop(256),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize((0.5), (0.5))\n",
    "                 ]\n",
    "                ) if not transform else transform\n",
    "\n",
    "        self.filenames = os.listdir(ROOT_DIR)\n",
    "        if \".DS_Store\" in self.filenames:\n",
    "            self.filenames.remove(\".DS_Store\")\n",
    "        self.ROOT_DIR = ROOT_DIR\n",
    "        self.random_slice = random_slice\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(repr(idx))\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if os.path.exists(os.path.join(self.ROOT_DIR, self.filenames[idx], f\"{self.filenames[idx]}.npy\")):\n",
    "            image = np.load(os.path.join(self.ROOT_DIR, self.filenames[idx], f\"{self.filenames[idx]}.npy\"))\n",
    "            pass\n",
    "        else:\n",
    "            img_name = os.path.join(\n",
    "                    self.ROOT_DIR, self.filenames[idx], f\"sub-{self.filenames[idx]}_ses-NFB3_T1w.nii.gz\"\n",
    "                    )\n",
    "            # random between 40 and 130\n",
    "            # print(nib.load(img_name).slicer[:,90:91,:].dataobj.shape)\n",
    "            img = nib.load(img_name)\n",
    "            image = img.get_fdata()\n",
    "\n",
    "            image_mean = np.mean(image)\n",
    "            image_std = np.std(image)\n",
    "            img_range = (image_mean - 1 * image_std, image_mean + 2 * image_std)\n",
    "            image = np.clip(image, img_range[0], img_range[1])\n",
    "            image = image / (img_range[1] - img_range[0])\n",
    "            np.save(\n",
    "                    os.path.join(self.ROOT_DIR, self.filenames[idx], f\"{self.filenames[idx]}.npy\"), image.astype(\n",
    "                            np.float32\n",
    "                            )\n",
    "                    )\n",
    "        if self.random_slice:\n",
    "            # slice_idx = randint(32, 122)\n",
    "            slice_idx = randint(40, 100)\n",
    "        else:\n",
    "            slice_idx = 80\n",
    "\n",
    "        image = image[:, slice_idx:slice_idx + 1, :].reshape(256, 192).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, \"filenames\": self.filenames[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efa9dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "das = MRIDataset(ROOT_DIR= r\"C:\\Users\\Admin\\Dropbox\\PC\\Documents\\Anomaly Detection\\AnoDDPM\\DATASETS\\Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d141c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for i in das:\n",
    "    print(i['image'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2670e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86681ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import blobfile as bf\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.fp16_util import MixedPrecisionTrainer\n",
    "from guided_diffusion.nn import update_ema\n",
    "from guided_diffusion.resample import LossAwareSampler, UniformSampler\n",
    "from visdom import Visdom\n",
    "viz = Visdom(port=8097)\n",
    "\n",
    "\n",
    "INITIAL_LOG_LOSS_SCALE = 20.0\n",
    "\n",
    "def visualize(img):\n",
    "    _min = img.min()\n",
    "    _max = img.max()\n",
    "    normalized_img = (img - _min)/ (_max - _min)\n",
    "    return normalized_img\n",
    "\n",
    "class TrainLoop:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model,\n",
    "        diffusion,\n",
    "        data,\n",
    "        batch_size,\n",
    "        microbatch,\n",
    "        lr,\n",
    "        ema_rate,\n",
    "        log_interval,\n",
    "        save_interval,\n",
    "        resume_checkpoint,\n",
    "        use_fp16=False,\n",
    "        fp16_scale_growth=1e-3,\n",
    "        schedule_sampler=None,\n",
    "        weight_decay=0.0,\n",
    "        lr_anneal_steps=0,\n",
    "        dataset='brats',\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.diffusion = diffusion\n",
    "        self.datal = data\n",
    "        self.dataset=dataset\n",
    "        self.iterdatal = iter(data)\n",
    "        self.batch_size = batch_size\n",
    "        self.microbatch = microbatch if microbatch > 0 else batch_size\n",
    "        self.lr = lr\n",
    "        self.ema_rate = (\n",
    "            [ema_rate]\n",
    "            if isinstance(ema_rate, float)\n",
    "            else [float(x) for x in ema_rate.split(\",\")]\n",
    "        )\n",
    "        self.log_interval = log_interval\n",
    "        self.save_interval = save_interval\n",
    "        self.resume_checkpoint = resume_checkpoint\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.fp16_scale_growth = fp16_scale_growth\n",
    "        self.schedule_sampler = schedule_sampler or UniformSampler(diffusion)\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_anneal_steps = lr_anneal_steps\n",
    "\n",
    "        self.step = 0\n",
    "        self.resume_step = 0\n",
    "        self.global_batch = self.batch_size\n",
    "\n",
    "        self.sync_cuda = th.cuda.is_available()\n",
    "\n",
    "        self._load_and_sync_parameters()\n",
    "        self.mp_trainer = MixedPrecisionTrainer(\n",
    "            model=self.model,\n",
    "            use_fp16=self.use_fp16,\n",
    "            fp16_scale_growth=fp16_scale_growth,\n",
    "        )\n",
    "\n",
    "        self.opt = AdamW(\n",
    "            self.mp_trainer.master_params, lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "        if self.resume_step:\n",
    "            self._load_optimizer_state()\n",
    "            # Model was resumed, either due to a restart or a checkpoint\n",
    "            # being specified at the command line.\n",
    "            self.ema_params = [\n",
    "                self._load_ema_parameters(rate) for rate in self.ema_rate\n",
    "            ]\n",
    "        else:\n",
    "            self.ema_params = [\n",
    "                copy.deepcopy(self.mp_trainer.master_params)\n",
    "                for _ in range(len(self.ema_rate))\n",
    "            ]\n",
    "\n",
    "        if th.cuda.is_available():\n",
    "            self.use_ddp = False\n",
    "            self.ddp_model = self.model.to(device)\n",
    "        else:\n",
    "#             if dist.get_world_size() > 1:\n",
    "#                 logger.warn(\n",
    "#                     \"Distributed training requires CUDA. \"\n",
    "#                     \"Gradients will not be synchronized properly!\"\n",
    "#                 )\n",
    "            self.use_ddp = False\n",
    "            self.ddp_model = self.model\n",
    "\n",
    "    def _load_and_sync_parameters(self):\n",
    "        resume_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n",
    "\n",
    "        if resume_checkpoint:\n",
    "            print('resume model')\n",
    "            self.resume_step = parse_resume_step_from_filename(resume_checkpoint)\n",
    "            if dist.get_rank() == 0:\n",
    "                logger.log(f\"loading model from checkpoint: {resume_checkpoint}...\")\n",
    "                self.model.load_state_dict(\n",
    "                    dist_util.load_state_dict(\n",
    "                        resume_checkpoint, map_location=dist_util.dev()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "#         dist_util.sync_params(self.model.parameters())\n",
    "\n",
    "    def _load_ema_parameters(self, rate):\n",
    "        ema_params = copy.deepcopy(self.mp_trainer.master_params)\n",
    "\n",
    "        main_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n",
    "        ema_checkpoint = find_ema_checkpoint(main_checkpoint, self.resume_step, rate)\n",
    "        if ema_checkpoint:\n",
    "            if dist.get_rank() == 0:\n",
    "                logger.log(f\"loading EMA from checkpoint: {ema_checkpoint}...\")\n",
    "                state_dict = dist_util.load_state_dict(\n",
    "                    ema_checkpoint, map_location=dist_util.dev()\n",
    "                )\n",
    "                ema_params = self.mp_trainer.state_dict_to_master_params(state_dict)\n",
    "\n",
    "        dist_util.sync_params(ema_params)\n",
    "        return ema_params\n",
    "\n",
    "    def _load_optimizer_state(self):\n",
    "        main_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n",
    "        opt_checkpoint = bf.join(\n",
    "            bf.dirname(main_checkpoint), f\"opt{self.resume_step:06}.pt\"\n",
    "        )\n",
    "        if bf.exists(opt_checkpoint):\n",
    "            logger.log(f\"loading optimizer state from checkpoint: {opt_checkpoint}\")\n",
    "            state_dict = dist_util.load_state_dict(\n",
    "                opt_checkpoint, map_location=dist_util.dev()\n",
    "            )\n",
    "            self.opt.load_state_dict(state_dict)\n",
    "\n",
    "    def run_loop(self):\n",
    "        i = 0\n",
    "\n",
    "        while (\n",
    "            not self.lr_anneal_steps\n",
    "            or self.step + self.resume_step < self.lr_anneal_steps\n",
    "        ):\n",
    "            if self.dataset=='brats':\n",
    "                try:\n",
    "                    batch, cond, label = next(self.iterdatal)\n",
    "                except:\n",
    "                    self.iterdatal = iter(self.datal)\n",
    "                    batch, cond, label, _, _ = next(self.iterdatal)\n",
    "            elif self.dataset=='chexpert':\n",
    "                batch, cond = next(self.datal)\n",
    "                cond.pop(\"path\", None)\n",
    "\n",
    "            self.run_step(batch, cond)\n",
    "\n",
    "            if self.step % self.log_interval == 0:\n",
    "                logger.dumpkvs()\n",
    "            if self.step % self.save_interval == 0:\n",
    "                self.save()\n",
    "                # Run for a finite amount of time in integration tests.\n",
    "                if os.environ.get(\"DIFFUSION_TRAINING_TEST\", \"\") and self.step > 0:\n",
    "                    return\n",
    "            self.step += 1\n",
    "        # Save the last checkpoint if it wasn't already saved.\n",
    "        if (self.step - 1) % self.save_interval != 0:\n",
    "            self.save()\n",
    "\n",
    "    def run_step(self, batch, cond):\n",
    "        lossmse,  sample = self.forward_backward(batch, cond)\n",
    "        took_step = self.mp_trainer.optimize(self.opt)\n",
    "        if took_step:\n",
    "            self._update_ema()\n",
    "        self._anneal_lr()\n",
    "        self.log_step()\n",
    "        return lossmse,  sample\n",
    "\n",
    "    def forward_backward(self, batch, cond):\n",
    "        self.mp_trainer.zero_grad()\n",
    "        for i in range(0, batch.shape[0], self.microbatch):\n",
    "            micro = batch[i : i + self.microbatch].to(dist_util.dev())\n",
    "            print('micro', micro.shape)\n",
    "            micro_cond = {\n",
    "                k: v[i : i + self.microbatch].to(dist_util.dev())\n",
    "                for k, v in cond.items()\n",
    "            }\n",
    "       \n",
    "            last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "            t, weights = self.schedule_sampler.sample(micro.shape[0], dist_util.dev())\n",
    "\n",
    "            compute_losses = functools.partial(\n",
    "                self.diffusion.training_losses,\n",
    "                self.ddp_model,\n",
    "                micro,\n",
    "                t,\n",
    "                model_kwargs=micro_cond,\n",
    "            )\n",
    "\n",
    "            if last_batch or not self.use_ddp:\n",
    "                losses1 = compute_losses()\n",
    "\n",
    "            else:\n",
    "                with self.ddp_model.no_sync():\n",
    "                    losses1 = compute_losses()\n",
    "\n",
    "            if isinstance(self.schedule_sampler, LossAwareSampler):\n",
    "                self.schedule_sampler.update_with_local_losses(\n",
    "                    t, losses[\"loss\"].detach()\n",
    "                )\n",
    "            losses = losses1[0]\n",
    "            sample = losses1[1]\n",
    "\n",
    "            loss = (losses[\"loss\"] * weights).mean()\n",
    "\n",
    "            lossmse = (losses[\"mse\"] * weights).mean().detach()\n",
    "           \n",
    "            log_loss_dict(\n",
    "                self.diffusion, t, {k: v * weights for k, v in losses.items()}\n",
    "            )\n",
    "            self.mp_trainer.backward(loss)\n",
    "\n",
    "            return lossmse.detach(),  sample\n",
    "\n",
    "    def _update_ema(self):\n",
    "        for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "            update_ema(params, self.mp_trainer.master_params, rate=rate)\n",
    "\n",
    "    def _anneal_lr(self):\n",
    "        if not self.lr_anneal_steps:\n",
    "            return\n",
    "        frac_done = (self.step + self.resume_step) / self.lr_anneal_steps\n",
    "        lr = self.lr * (1 - frac_done)\n",
    "        for param_group in self.opt.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def log_step(self):\n",
    "        logger.logkv(\"step\", self.step + self.resume_step)\n",
    "        logger.logkv(\"samples\", (self.step + self.resume_step + 1) * self.global_batch)\n",
    "\n",
    "    def save(self):\n",
    "        def save_checkpoint(rate, params):\n",
    "            state_dict = self.mp_trainer.master_params_to_state_dict(params)\n",
    "            if dist.get_rank() == 0:\n",
    "                logger.log(f\"saving model {rate}...\")\n",
    "                if not rate:\n",
    "                    filename = f\"brats2update{(self.step+self.resume_step):06d}.pt\"\n",
    "                else:\n",
    "                    filename = f\"emabrats2update_{rate}_{(self.step+self.resume_step):06d}.pt\"\n",
    "                print('filename', filename)\n",
    "                with bf.BlobFile(bf.join(get_blob_logdir(), filename), \"wb\") as f:\n",
    "                    th.save(state_dict, f)\n",
    "\n",
    "        save_checkpoint(0, self.mp_trainer.master_params)\n",
    "        for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "            save_checkpoint(rate, params)\n",
    "\n",
    "        if dist.get_rank() == 0:\n",
    "            with bf.BlobFile(\n",
    "                bf.join(get_blob_logdir(), f\"optbrats2update{(self.step+self.resume_step):06d}.pt\"),\n",
    "                \"wb\",\n",
    "            ) as f:\n",
    "                th.save(self.opt.state_dict(), f)\n",
    "\n",
    "        dist.barrier()\n",
    "\n",
    "\n",
    "def parse_resume_step_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse filenames of the form path/to/modelNNNNNN.pt, where NNNNNN is the\n",
    "    checkpoint's number of steps.\n",
    "    \"\"\"\n",
    "    split = filename.split(\"model\")\n",
    "    if len(split) < 2:\n",
    "        return 0\n",
    "    split1 = split[-1].split(\".\")[0]\n",
    "    try:\n",
    "        return int(split1)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_blob_logdir():\n",
    "    # You can change this to be a separate path to save checkpoints to\n",
    "    # a blobstore or some external drive.\n",
    "    return logger.get_dir()\n",
    "\n",
    "\n",
    "def find_resume_checkpoint():\n",
    "    # On your infrastructure, you may want to override this to automatically\n",
    "    # discover the latest checkpoint on your blob storage, etc.\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_ema_checkpoint(main_checkpoint, step, rate):\n",
    "    if main_checkpoint is None:\n",
    "        return None\n",
    "    filename = f\"ema_{rate}_{(step):06d}.pt\"\n",
    "    path = bf.join(bf.dirname(main_checkpoint), filename)\n",
    "    if bf.exists(path):\n",
    "        return path\n",
    "    return None\n",
    "\n",
    "\n",
    "def log_loss_dict(diffusion, ts, losses):\n",
    "    for key, values in losses.items():\n",
    "        logger.logkv_mean(key, values.mean().item())\n",
    "        # Log the quantiles (four quartiles, in particular).\n",
    "        for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
    "            quartile = int(4 * sub_t / diffusion.num_timesteps)\n",
    "            logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f3e4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to C:\\Users\\Admin\\AppData\\Local\\Temp\\openai-2023-02-17-15-21-57-411662\n",
      "creating data loader...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 164\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     batch, cond, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdatal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[13], line 71\u001b[0m, in \u001b[0;36mBRATSDataset.__getitem__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mout[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\t\t\u001b[38;5;66;03m#pad to a size of (256,256)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m label \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 98\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 54\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset is chexpert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#     logger.log(\"training...\")\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mTrainLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmicrobatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmicrobatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp16_scale_growth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschedule_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_anneal_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 167\u001b[0m, in \u001b[0;36mTrainLoop.run_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterdatal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatal)\n\u001b[1;32m--> 167\u001b[0m         batch, cond, label, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdatal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchexpert\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    169\u001b[0m     batch, cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatal)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 71\u001b[0m, in \u001b[0;36mBRATSDataset.__getitem__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mout[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\t\t\u001b[38;5;66;03m#pad to a size of (256,256)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     label \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a diffusion model on images.\n",
    "\"\"\"\n",
    "import sys\n",
    "import argparse\n",
    "import torch as th\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\".\")\n",
    "# from guided_diffusion.bratsloader import BRATSDataset\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.image_datasets import load_data\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "from guided_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    args_to_dict,\n",
    "    add_dict_to_argparser,\n",
    ")\n",
    "# from guided_diffusion.train_util import TrainLoop\n",
    "from visdom import Visdom\n",
    "viz = Visdom(port=8097)\n",
    "\n",
    "def main():\n",
    "#     args = create_argparser().parse_args()\n",
    "\n",
    "#     dist_util.setup_dist()\n",
    "#     logger.configure()\n",
    "\n",
    "#     logger.log(\"creating model and diffusion...\")\n",
    "    model, diffusion = create_model_and_diffusion(args)\n",
    "    model.to(dist_util.dev())\n",
    "    schedule_sampler = create_named_schedule_sampler(args.schedule_sampler, diffusion,  maxt=1000)\n",
    "\n",
    "    logger.log(\"creating data loader...\")\n",
    "\n",
    "    if args.dataset == 'brats':\n",
    "        ds = BRATSDataset(args.data_dir, test_flag=False)\n",
    "        datal = th.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True)\n",
    "       # data = iter(datal)\n",
    "\n",
    "    elif args.dataset == 'chexpert':\n",
    "        datal = load_data(\n",
    "            data_dir=args.data_dir,\n",
    "            batch_size=1,\n",
    "            image_size=args.image_size,\n",
    "            class_cond=True,\n",
    "        )\n",
    "        print('dataset is chexpert')\n",
    "\n",
    "#     logger.log(\"training...\")\n",
    "    TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=datal,\n",
    "        batch_size=args.batch_size,\n",
    "        microbatch=args.microbatch,\n",
    "        lr=args.lr,\n",
    "        ema_rate=args.ema_rate,\n",
    "        log_interval=args.log_interval,\n",
    "        save_interval=args.save_interval,\n",
    "        resume_checkpoint=args.resume_checkpoint,\n",
    "        use_fp16=args.use_fp16,\n",
    "        fp16_scale_growth=args.fp16_scale_growth,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=args.weight_decay,\n",
    "        lr_anneal_steps=args.lr_anneal_steps,\n",
    "        dataset=args.dataset\n",
    "    ).run_loop()\n",
    "\n",
    "\n",
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        data_dir=\"\",\n",
    "        schedule_sampler=\"uniform\",\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.0,\n",
    "        lr_anneal_steps=0,\n",
    "        batch_size=1,\n",
    "        microbatch=-1,  # -1 disables microbatches\n",
    "        ema_rate=\"0.9999\",  # comma-separated list of EMA values\n",
    "        log_interval=100,\n",
    "        save_interval=10000,\n",
    "        resume_checkpoint='',\n",
    "        use_fp16=False,\n",
    "        fp16_scale_growth=1e-3,\n",
    "        dataset='brats',\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_dict_to_argparser(parser, defaults)\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3c3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff60a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BRATSDataset(args.data_dir, test_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d69924f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datal = th.utils.data.DataLoader(ds, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7063944",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m datal:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 71\u001b[0m, in \u001b[0;36mBRATSDataset.__getitem__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39mout[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\t\t\u001b[38;5;66;03m#pad to a size of (256,256)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     label \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.DoubleTensor{[4, 240, 240, 155]}, size=[4, 240, 240]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (4)"
     ]
    }
   ],
   "source": [
    "for i in datal:\n",
    "    print(i[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4066ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
