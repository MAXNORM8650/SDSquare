{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665cb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "from random import seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from torch import optim\n",
    "from guided_diffusion.fp16_util import MixedPrecisionTrainer\n",
    "import dataset\n",
    "import evaluation\n",
    "from GaussianDiffusion import GaussianDiffusionModel, get_beta_schedule\n",
    "from helpers import *\n",
    "from UNet import UNetModel, update_ema_params\n",
    "from script_util import (\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    classifier_and_diffusion_defaults,\n",
    "    create_classifier_and_diffusion,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# from train_util import  log_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98bdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43c01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import blobfile as bf\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.fp16_util import MixedPrecisionTrainer\n",
    "from guided_diffusion.image_datasets import load_data\n",
    "from guided_diffusion.resample import create_named_schedule_sampler\n",
    "from script_util import (\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    classifier_and_diffusion_defaults,\n",
    "    create_classifier_and_diffusion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e32faedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "  \"image_size\": 64,\n",
    "  \"Batch_Size\": 1,\n",
    "  \"channels\": 3,\n",
    "  \"EPOCHS\": 4000,\n",
    "  \"diffusion_steps\": 1000,\n",
    "  \"num_channels\": 192,\\\n",
    "  \"learn_sigma\":True,\n",
    "  \"microbatch\": -1,\n",
    "  \"use_kl\":False,\n",
    "  \"schedule_sampler\": \"uniform\",\n",
    "  \"predict_xstart\":False,\n",
    "  \"rescale_timesteps\":False,\n",
    "  \"rescale_learned_sigmas\":False,\n",
    "  \"timestep_respacing\": \"\",\n",
    "  \"noise_schedule\": \"cosine\",\n",
    "  \"channel_mults\": \"\",\n",
    "  \"loss-type\": \"l2\",\n",
    "  \"loss_weight\": \"none\",\n",
    "  \"train_start\": True,\n",
    "  \"lr\": 1e-4,\n",
    "  \"learn_sigma \": True,\n",
    "  \"random_slice\": True,\n",
    "  \"sample_distance\": 800,\n",
    "  \"weight_decay\": 0.0,\n",
    "  \"save_imgs\": True,\n",
    "  \"save_vids\": True,\n",
    "  \"class_cond \": True,\n",
    "  \"use_fp16\": True,\n",
    "  \"use_scale_shift_norm\": True,\n",
    "  \"dropout\": 0.1,\n",
    "  \"attention_resolutions\": \"32,16,8\",\n",
    "  \"num_res_blocks\": 3,\n",
    "  \"resblock_updown\": True,\n",
    "  \"classifier_use_fp16\":False,\n",
    "  \"classifier_width\":128,\n",
    "  \"classifier_depth\": 2,\n",
    "  \"classifier_attention_resolutions\": \"32,16,8\",\n",
    "  \"classifier_use_scale_shift_norm\": True,\n",
    "  \"classifier_resblock_updown\": True,\n",
    "  \"classifier_pool\": \"attention\" ,\n",
    "  \"num_head_channels\": 64,\n",
    "  \"noised\": True,\n",
    "  \"noise_fn\": \"gauss\",\n",
    "  \"dataset\": \"mura\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc4180e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed405a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, diffusion = create_classifier_and_diffusion(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edb5fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b35c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07181805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4011c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_trainer = MixedPrecisionTrainer(\n",
    "#     model=model, use_fp16=args['classifier_use_fp16'], initial_lg_loss_scale=16.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5484fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "004cf67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_dict(diffusion, ts, losses):\n",
    "    for key, values in losses.items():\n",
    "        logger.logkv_mean(key, values.mean().item())\n",
    "        # Log the quantiles (four quartiles, in particular).\n",
    "        for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
    "            quartile = int(4 * sub_t / diffusion.num_timesteps)\n",
    "            logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b948ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./\"\n",
    "\n",
    "\n",
    "def train(training_dataset_loader, testing_dataset_loader, args, resume):\n",
    "    \"\"\"\n",
    "\n",
    "    :param training_dataset_loader: cycle(dataloader) instance for training\n",
    "    :param testing_dataset_loader:  cycle(dataloader) instance for testing\n",
    "    :param args: dictionary of parameters\n",
    "    :param resume: dictionary of parameters if continuing training from checkpoint\n",
    "    :return: Trained model and tested\n",
    "    \"\"\"\n",
    "    logger.configure()\n",
    "\n",
    "    logger.log(\"creating model and diffusion...\")\n",
    "    in_channels = 4\n",
    "    if args[\"dataset\"].lower() == \"cifar\" or args[\"dataset\"].lower() == \"leather\":\n",
    "        in_channels = 3\n",
    "    if args[\"dataset\"].lower() == \"brats\":\n",
    "        in_channels = 4\n",
    "    if args[\"channels\"] != \"\":\n",
    "        in_channels = args[\"channels\"]\n",
    "    model, diffusion = create_classifier_and_diffusion(args)\n",
    "    if resume:\n",
    "        model.load_state_dict(resume[\"unet\"])\n",
    "        start_epoch = resume['n_epoch']\n",
    "\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        ema = copy.deepcopy(model)\n",
    "    mp_trainer = MixedPrecisionTrainer(\n",
    "        model=model, use_fp16=args['classifier_use_fp16'], initial_lg_loss_scale=16.0\n",
    "    )\n",
    "    tqdm_epoch = range(start_epoch, args['EPOCHS'] + 1)\n",
    "    model.to(device)\n",
    "    if args[\"noised\"]:\n",
    "        schedule_sampler = create_named_schedule_sampler(\n",
    "            args[\"schedule_sampler\"], diffusion\n",
    "        )\n",
    "\n",
    "    resume_step = 0\n",
    "    logger.log(f\"creating optimizer...\")\n",
    "    optimiser = optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'], betas=(0.9, 0.999))\n",
    "    if resume:\n",
    "#         logger.log(f\"loading optimizer state from checkpoint: {resume[\"optimizer_state_dict\"]}\")\n",
    "        optimiser.load_state_dict(resume[\"optimizer_state_dict\"])\n",
    "    del resume\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    vlb = collections.deque([], maxlen=10)\n",
    "    iters = range(100 // args['Batch_Size']) if args[\"dataset\"].lower() != \"cifar\" else range(200)\n",
    "    # iters = range(100 // args['Batch_Size']) if args[\"dataset\"].lower() != \"cifar\" else range(150)\n",
    "\n",
    "    # dataset loop\n",
    "    for epoch in tqdm_epoch:\n",
    "        \n",
    "        mean_loss = []\n",
    "#         if args[\"anneal_lr\"]:\n",
    "#             set_annealed_lr(opt, args[\"lr\"], (step + resume_step) / args.iterations)\n",
    "        for i in iters:\n",
    "            data = next(training_dataset_loader)\n",
    "            if args[\"dataset\"] == \"cifar\":\n",
    "                # cifar outputs [data,class]\n",
    "                batch = data[0].to(device)\n",
    "            else:\n",
    "                batch = data[\"image\"]\n",
    "                batch = batch.to(device)\n",
    "                labels = data[\"label\"].to(device)\n",
    "#             print(batch.shape)\n",
    "            if args[\"noised\"]:\n",
    "                t, _ = schedule_sampler.sample(batch.shape[0], device)\n",
    "                batch = diffusion.q_sample(batch, t)\n",
    "            \n",
    "            else:\n",
    "                t = th.zeros(batch.shape[0], dtype=th.long, device=device).repeat(batch.shape[0])\n",
    "#             print(batch.shape)\n",
    "#             print(t.shape)\n",
    "            for i, (sub_batch, sub_labels, sub_t) in enumerate(split_microbatches(args['microbatch'], batch, labels, t)):\n",
    "                logits = model(sub_batch, timesteps=sub_t)\n",
    "                loss = F.cross_entropy(logits, sub_labels, reduction=\"none\")\n",
    "                losses = {}\n",
    "                losses[f\"{prefix}_loss\"] = loss.detach()\n",
    "                losses[f\"{prefix}_acc@1\"] = compute_top_k(logits, sub_labels, k=1, reduction=\"none\")\n",
    "                losses[f\"{prefix}_acc@5\"] = compute_top_k(logits, sub_labels, k=5, reduction=\"none\")\n",
    "                log_loss_dict(diffusion, sub_t, losses)\n",
    "                del losses\n",
    "                loss = loss.mean()\n",
    "                if loss.requires_grad:\n",
    "                    if i == 0:\n",
    "                        mp_trainer.zero_grad()\n",
    "                    mp_trainer.backward(loss * len(sub_batch) / len(batch))\n",
    "            mp_trainer.optimize(optimiser)\n",
    "            mean_loss.append(loss.data.cpu())\n",
    "        mean_loss = np.mean(mean_loss)\n",
    "        print(\"epoch\", epoch, mean_loss)\n",
    "\n",
    "#             if epoch % 50 == 0 and i == 0:\n",
    "#                 row_size = min(8, args['Batch_Size'])\n",
    "#                 training_outputs(\n",
    "#                         diffusion, x, est, noisy, epoch, row_size, save_imgs=args['save_imgs'],\n",
    "#                         save_vids=args['save_vids'], ema=ema, args=args\n",
    "#                         )\n",
    "\n",
    "#         losses.append(np.mean(mean_loss))\n",
    "#         if epoch % 200 == 0:\n",
    "#             time_taken = time.time() - start_time\n",
    "#             remaining_epochs = args['EPOCHS'] - epoch\n",
    "#             time_per_epoch = time_taken / (epoch + 1 - start_epoch)\n",
    "#             hours = remaining_epochs * time_per_epoch / 3600\n",
    "#             mins = (hours % 1) * 60\n",
    "#             hours = int(hours)\n",
    "\n",
    "#             vlb_terms = diffusion.calc_total_vlb(x, model, args)\n",
    "#             vlb.append(vlb_terms[\"total_vlb\"].mean(dim=-1).cpu().item())\n",
    "#             print(\n",
    "#                     f\"epoch: {epoch}, most recent total VLB: {vlb[-1]} mean total VLB:\"\n",
    "#                     f\" {np.mean(vlb):.4f}, \"\n",
    "#                     f\"prior vlb: {vlb_terms['prior_vlb'].mean(dim=-1).cpu().item():.2f}, vb: \"\n",
    "#                     f\"{torch.mean(vlb_terms['vb'], dim=list(range(2))).cpu().item():.2f}, x_0_mse: \"\n",
    "#                     f\"{torch.mean(vlb_terms['x_0_mse'], dim=list(range(2))).cpu().item():.2f}, mse: \"\n",
    "#                     f\"{torch.mean(vlb_terms['mse'], dim=list(range(2))).cpu().item():.2f}\"\n",
    "#                     f\" time elapsed {int(time_taken / 3600)}:{((time_taken / 3600) % 1) * 60:02.0f}, \"\n",
    "#                     f\"est time remaining: {hours}:{mins:02.0f}\\r\"\n",
    "#                     )\n",
    "#             # else:\n",
    "#             #\n",
    "#             #     print(\n",
    "#             #             f\"epoch: {epoch}, imgs trained: {(i + 1) * args['Batch_Size'] + epoch * 100}, last 20 epoch mean loss:\"\n",
    "#             #             f\" {np.mean(losses[-20:]):.4f} , last 100 epoch mean loss:\"\n",
    "#             #             f\" {np.mean(losses[-100:]) if len(losses) > 0 else 0:.4f}, \"\n",
    "#             #             f\"time per epoch {time_per_epoch:.2f}s, time elapsed {int(time_taken / 3600)}:\"\n",
    "#             #             f\"{((time_taken / 3600) % 1) * 60:02.0f}, est time remaining: {hours}:{mins:02.0f}\\r\"\n",
    "#             #             )\n",
    "\n",
    "#         if epoch % 1000 == 0 and epoch >= 0:\n",
    "#             save(unet=model, args=args, optimiser=optimiser, final=False, epoch=epoch)\n",
    "\n",
    "#     save(unet=model, args=args, optimiser=optimiser, final=True)\n",
    "\n",
    "#     evaluation.testing(testing_dataset_loader, diffusion, ema=ema, args=args, model=model)\n",
    "\n",
    "\n",
    "def save(final, unet, optimiser, args, loss=0, epoch=0):\n",
    "    \"\"\"\n",
    "    Save model final or checkpoint\n",
    "    :param final: bool for final vs checkpoint\n",
    "    :param unet: unet instance\n",
    "    :param optimiser: ADAM optim\n",
    "    :param args: model parameters\n",
    "    :param ema: ema instance\n",
    "    :param loss: loss for checkpoint\n",
    "    :param epoch: epoch for checkpoint\n",
    "    :return: saved model\n",
    "    \"\"\"\n",
    "    if final:\n",
    "        torch.save(\n",
    "                {\n",
    "                    'n_epoch':              args[\"EPOCHS\"],\n",
    "                    'model_state_dict':     unet.state_dict(),\n",
    "                    'optimizer_state_dict': optimiser.state_dict(),\n",
    "                    \"args\":                 args\n",
    "                    # 'loss': LOSS,\n",
    "                    }, f'{ROOT_DIR}model/diff-params-ARGS={args[\"arg_num\"]}/params-final.pt'\n",
    "                )\n",
    "    else:\n",
    "        torch.save(\n",
    "                {\n",
    "                    'n_epoch':              epoch,\n",
    "                    'model_state_dict':     unet.state_dict(),\n",
    "                    'optimizer_state_dict': optimiser.state_dict(),\n",
    "                    \"args\":                 args,\n",
    "                    'loss':                 loss,\n",
    "                    }, f'{ROOT_DIR}model/diff-params-ARGS={args[\"arg_num\"]}/checkpoint/diff_epoch={epoch}.pt'\n",
    "                )\n",
    "\n",
    "\n",
    "def training_outputs(diffusion, x, est, noisy, epoch, row_size, ema, args, save_imgs=False, save_vids=False):\n",
    "    \"\"\"\n",
    "    Saves video & images based on args info\n",
    "    :param diffusion: diffusion model instance\n",
    "    :param x: x_0 real data value\n",
    "    :param est: estimate of the noise at x_t (output of the model)\n",
    "    :param noisy: x_t\n",
    "    :param epoch:\n",
    "    :param row_size: rows for outputs into torchvision.utils.make_grid\n",
    "    :param ema: exponential moving average unet for sampling\n",
    "    :param save_imgs: bool for saving imgs\n",
    "    :param save_vids: bool for saving diffusion videos\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(f'./diffusion-videos/ARGS={args[\"arg_num\"]}')\n",
    "        os.makedirs(f'./diffusion-training-images/ARGS={args[\"arg_num\"]}')\n",
    "    except OSError:\n",
    "        pass\n",
    "    if save_imgs:\n",
    "        if epoch % 100 == 0:\n",
    "            # for a given t, output x_0, & prediction of x_(t-1), and x_0\n",
    "            noise = torch.rand_like(x)\n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=x.device)\n",
    "            x_t = diffusion.sample_q(x, t, noise)\n",
    "            temp = diffusion.sample_p(ema, x_t, t)\n",
    "            out = torch.cat(\n",
    "                    (x[:row_size, ...].cpu(), temp[\"sample\"][:row_size, ...].cpu(),\n",
    "                     temp[\"pred_x_0\"][:row_size, ...].cpu())\n",
    "                    )\n",
    "            plt.title(f'real, sample, prediction x_0-{epoch}epoch')\n",
    "        else:\n",
    "            # for a given t, output x_0, x_t, & prediction of noise in x_t & MSE\n",
    "            out = torch.cat(\n",
    "                    (x[:row_size, ...].cpu(), noisy[:row_size, ...].cpu(), est[:row_size, ...].cpu(),\n",
    "                     (est - noisy).square().cpu()[:row_size, ...])\n",
    "                    )\n",
    "            plt.title(f'real,noisy,noise prediction,mse-{epoch}epoch')\n",
    "        plt.rcParams['figure.dpi'] = 150\n",
    "        plt.grid(False)\n",
    "        plt.imshow(gridify_output(out, row_size), cmap='gray')\n",
    "\n",
    "        plt.savefig(f'./diffusion-training-images/ARGS={args[\"arg_num\"]}/EPOCH={epoch}.png')\n",
    "        plt.clf()\n",
    "    if save_vids:\n",
    "        fig, ax = plt.subplots()\n",
    "        if epoch % 500 == 0:\n",
    "            plt.rcParams['figure.dpi'] = 200\n",
    "            if epoch % 1000 == 0:\n",
    "                out = diffusion.forward_backward(ema, x, \"half\", args['sample_distance'] // 2, denoise_fn=\"noise_fn\")\n",
    "            else:\n",
    "                out = diffusion.forward_backward(ema, x, \"half\", args['sample_distance'] // 4, denoise_fn=\"noise_fn\")\n",
    "            imgs = [[ax.imshow(gridify_output(x, row_size), animated=True)] for x in out]\n",
    "            ani = animation.ArtistAnimation(\n",
    "                    fig, imgs, interval=50, blit=True,\n",
    "                    repeat_delay=1000\n",
    "                    )\n",
    "\n",
    "            ani.save(f'{ROOT_DIR}diffusion-videos/ARGS={args[\"arg_num\"]}/sample-EPOCH={epoch}.gif')            \n",
    "\n",
    "    plt.close('all')\n",
    "def set_annealed_lr(opt, base_lr, frac_done):\n",
    "    lr = base_lr * (1 - frac_done)\n",
    "    for param_group in opt.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def save_model(mp_trainer, opt, step):\n",
    "    if dist.get_rank() == 0:\n",
    "        th.save(\n",
    "            mp_trainer.master_params_to_state_dict(mp_trainer.master_params),\n",
    "            os.path.join(logger.get_dir(), f\"model{step:06d}.pt\"),\n",
    "        )\n",
    "        th.save(opt.state_dict(), os.path.join(logger.get_dir(), f\"opt{step:06d}.pt\"))\n",
    "\n",
    "\n",
    "def compute_top_k(logits, labels, k, reduction=\"mean\"):\n",
    "    _, top_ks = th.topk(logits, k, dim=-1)\n",
    "    if reduction == \"mean\":\n",
    "        return (top_ks == labels[:, None]).float().sum(dim=-1).mean().item()\n",
    "    elif reduction == \"none\":\n",
    "        return (top_ks == labels[:, None]).float().sum(dim=-1)\n",
    "\n",
    "\n",
    "def split_microbatches(microbatch, *args):\n",
    "    bs = len(args[0])\n",
    "    if microbatch == -1 or microbatch >= bs:\n",
    "        yield tuple(args)\n",
    "    else:\n",
    "        for i in range(0, bs, microbatch):\n",
    "            yield tuple(x[i : i + microbatch] if x is not None else None for x in args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ac0c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume =False\n",
    "training_dataset, testing_dataset = dataset.init_datasets(ROOT_DIR, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362e8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d66b0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_loader = dataset.init_dataset_loader(training_dataset, args)\n",
    "testing_dataset_loader = dataset.init_dataset_loader(testing_dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93825693",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "prefix = \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "855491bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to C:\\Users\\Admin\\AppData\\Local\\Temp\\openai-2023-02-15-18-19-19-640687\n",
      "creating model and diffusion...\n",
      "creating optimizer...\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "epoch 0 0.35991755\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataset_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_dataset_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 92\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 mp_trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     91\u001b[0m             mp_trainer\u001b[38;5;241m.\u001b[39mbackward(loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sub_batch) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch))\n\u001b[1;32m---> 92\u001b[0m     \u001b[43mmp_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     mean_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     94\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mean_loss)\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Anomaly Detection\\AnoDDPM\\guided_diffusion\\fp16_util.py:187\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer.optimize\u001b[1;34m(self, opt)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimize_fp16(opt)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Anomaly Detection\\AnoDDPM\\guided_diffusion\\fp16_util.py:211\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer._optimize_normal\u001b[1;34m(self, opt)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_optimize_normal\u001b[39m(\u001b[38;5;28mself\u001b[39m, opt: th\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m--> 211\u001b[0m     grad_norm, param_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_norms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlogkv_mean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, grad_norm)\n\u001b[0;32m    213\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlogkv_mean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, param_norm)\n",
      "File \u001b[1;32m~\\Dropbox\\PC\\Documents\\Anomaly Detection\\AnoDDPM\\guided_diffusion\\fp16_util.py:224\u001b[0m, in \u001b[0;36mMixedPrecisionTrainer._compute_norms\u001b[1;34m(self, grad_scale)\u001b[0m\n\u001b[0;32m    222\u001b[0m         param_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mnorm(p, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mth\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m             grad_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(grad_norm) \u001b[38;5;241m/\u001b[39m grad_scale, np\u001b[38;5;241m.\u001b[39msqrt(param_norm)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\en_2\\lib\\site-packages\\torch\\functional.py:1539\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28minput\u001b[39m, p, _dim, keepdim\u001b[38;5;241m=\u001b[39mkeepdim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(training_dataset_loader, testing_dataset_loader, args, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a6c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb83c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881c724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934a4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0692ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55212d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bed465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d9ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed(1)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"\n",
    "        Load arguments, run training and testing functions, then remove checkpoint directory\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # make directories\n",
    "    for i in ['./model/', \"./diffusion-videos/\", './diffusion-training-images/']:\n",
    "        try:\n",
    "            os.makedirs(i)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    # read file from argument\n",
    "#     if len(sys.argv[1:]) > 0:\n",
    "#         files = sys.argv[1:]\n",
    "#     else:\n",
    "#         raise ValueError(\"Missing file argument\")\n",
    "\n",
    "#     # resume from final or resume from most recent checkpoint -> ran from specific slurm script?\n",
    "#     resume = 0\n",
    "#     if files[0] == \"RESUME_RECENT\":\n",
    "#         resume = 1\n",
    "#         files = files[1:]\n",
    "#         if len(files) == 0:\n",
    "#             raise ValueError(\"Missing file argument\")\n",
    "#     elif files[0] == \"RESUME_FINAL\":\n",
    "#         resume = 2\n",
    "#         files = files[1:]\n",
    "#         if len(files) == 0:\n",
    "#             raise ValueError(\"Missing file argument\")\n",
    "\n",
    "#     # allow different arg inputs ie 25 or args15 which are converted into argsNUM.json\n",
    "#     file = files[0]\n",
    "#     if file.isnumeric():\n",
    "#         file = f\"args{file}.json\"\n",
    "#     elif file[:4] == \"args\" and file[-5:] == \".json\":\n",
    "#         pass\n",
    "#     elif file[:4] == \"args\":\n",
    "#         file = f\"args{file[4:]}.json\"\n",
    "#     else:\n",
    "#         raise ValueError(\"File Argument is not a json file\")\n",
    "\n",
    "#     # load the json args\n",
    "#     with open(f'{ROOT_DIR}test_args/{file}', 'r') as f:\n",
    "#         args = json.load(f)\n",
    "#     args['arg_num'] = file[4:-5]\n",
    "    args = defaultdict_from_json(args)\n",
    "\n",
    "    # make arg specific directories\n",
    "    for i in [f'./model/diff-params-ARGS={args[\"arg_num\"]}',\n",
    "              f'./model/diff-params-ARGS={args[\"arg_num\"]}/checkpoint',\n",
    "              f'./diffusion-videos/ARGS={args[\"arg_num\"]}',\n",
    "              f'./diffusion-training-images/ARGS={args[\"arg_num\"]}']:\n",
    "        try:\n",
    "            os.makedirs(i)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "#     print(file, args)\n",
    "    if args[\"channels\"] != \"\":\n",
    "        in_channels = args[\"channels\"]\n",
    "\n",
    "    # if dataset is cifar, load different training & test set\n",
    "    if args[\"dataset\"].lower() == \"cifar\":\n",
    "        training_dataset_loader_, testing_dataset_loader_ = dataset.load_CIFAR10(args, True), \\\n",
    "                                                            dataset.load_CIFAR10(args, False)\n",
    "        training_dataset_loader = dataset.cycle(training_dataset_loader_)\n",
    "        testing_dataset_loader = dataset.cycle(testing_dataset_loader_)\n",
    "    elif args[\"dataset\"].lower() == \"carpet\":\n",
    "        training_dataset = dataset.DAGM(\n",
    "                \"./DATASETS/CARPET/Class1\", False, args[\"img_size\"],\n",
    "                False\n",
    "                )\n",
    "        training_dataset_loader = dataset.init_dataset_loader(training_dataset, args)\n",
    "        testing_dataset = dataset.DAGM(\n",
    "                \"./DATASETS/CARPET/Class1\", True, args[\"img_size\"],\n",
    "                False\n",
    "                )\n",
    "        testing_dataset_loader = dataset.init_dataset_loader(testing_dataset, args)\n",
    "    elif args[\"dataset\"].lower() == \"leather\":\n",
    "        if in_channels == 3:\n",
    "            training_dataset = dataset.MVTec(\n",
    "                    \"./DATASETS/leather\", anomalous=False, img_size=args[\"img_size\"],\n",
    "                    rgb=True\n",
    "                    )\n",
    "            testing_dataset = dataset.MVTec(\n",
    "                    \"./DATASETS/leather\", anomalous=True, img_size=args[\"img_size\"],\n",
    "                    rgb=True, include_good=True\n",
    "                    )\n",
    "        else:\n",
    "            training_dataset = dataset.MVTec(\n",
    "                    \"./DATASETS/leather\", anomalous=False, img_size=args[\"img_size\"],\n",
    "                    rgb=False\n",
    "                    )\n",
    "            testing_dataset = dataset.MVTec(\n",
    "                    \"./DATASETS/leather\", anomalous=True, img_size=args[\"img_size\"],\n",
    "                    rgb=False, include_good=True\n",
    "                    )\n",
    "        training_dataset_loader = dataset.init_dataset_loader(training_dataset, args)\n",
    "        testing_dataset_loader = dataset.init_dataset_loader(testing_dataset, args)\n",
    "    else:\n",
    "        # load NFBS dataset\n",
    "        training_dataset, testing_dataset = dataset.init_datasets(ROOT_DIR, args)\n",
    "        training_dataset_loader = dataset.init_dataset_loader(training_dataset, args)\n",
    "        testing_dataset_loader = dataset.init_dataset_loader(testing_dataset, args)\n",
    "\n",
    "    # if resuming, loaded model is attached to the dictionary\n",
    "    loaded_model = {}\n",
    "    if resume:\n",
    "        if resume == 1:\n",
    "            checkpoints = os.listdir(f'./model/diff-params-ARGS={args[\"arg_num\"]}/checkpoint')\n",
    "            checkpoints.sort(reverse=True)\n",
    "            for i in checkpoints:\n",
    "                try:\n",
    "                    file_dir = f\"./model/diff-params-ARGS={args['arg_num']}/checkpoint/{i}\"\n",
    "                    loaded_model = torch.load(file_dir, map_location=device)\n",
    "                    break\n",
    "                except RuntimeError:\n",
    "                    continue\n",
    "\n",
    "        else:\n",
    "            file_dir = f'./model/diff-params-ARGS={args[\"arg_num\"]}/params-final.pt'\n",
    "            loaded_model = torch.load(file_dir, map_location=device)\n",
    "\n",
    "    # load, pass args\n",
    "    train(training_dataset_loader, testing_dataset_loader, args, loaded_model)\n",
    "\n",
    "    # remove checkpoints after final_param is saved (due to storage requirements)\n",
    "    for file_remove in os.listdir(f'./model/diff-params-ARGS={args[\"arg_num\"]}/checkpoint'):\n",
    "        os.remove(os.path.join(f'./model/diff-params-ARGS={args[\"arg_num\"]}/checkpoint', file_remove))\n",
    "    os.removedirs(f'./model/diff-params-ARGS={args[\"arg_num\"]}/checkpoint')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
